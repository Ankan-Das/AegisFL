**You should be able to answer:**
- What are neighboring datasets?
- What do Îµ and Î´ mean (in one sentence)?
- What is sensitivity and how does it set the noise?
- What are Laplace/Gaussian mechanisms?
- What is postâ€‘processing? What is composition?


# ğŸ› ï¸ Chapter 2

Chapter 2 is about the **tools** that make Differential Privacy (DP) actually work.  
Think of this as your **DP toolbox**.

---

## 1. Global Sensitivity

ğŸ‘‰ **Definition:**  
Sensitivity = the **maximum change** in a functionâ€™s output if you change or remove just **one personâ€™s data**.

ğŸ‘‰ **Why it matters:**  
If one person can swing the result a lot, you need **more noise** to hide them.  
If one person barely affects it, you can add **less noise**.

**Examples:**
- **Counting people:** sensitivity = 1 (one person can change the count by at most 1).  
- **Average rating (0â€“5 stars, n people):** sensitivity = 5/n (one person changing from 0 â†’ 5 changes the mean by at most 5/n).  

---

## 2. Laplace Mechanism (Pure DP, Î´=0)

ğŸ‘‰ **Definition:**  
Add noise from a **Laplace distribution** with scale:

\[
b = \frac{\text{sensitivity}}{\varepsilon}
\]

ğŸ‘‰ **Meaning in practice:**  
- Compute your answer (e.g., â€œ42 people like pizzaâ€).  
- Add Laplace noise.  
- Output might be 41.2 or 43.7.  

**Takeaway:** Smaller Îµ â†’ bigger noise â†’ stronger privacy.

---

## 3. Gaussian Mechanism (Approximate DP, Î´>0)

ğŸ‘‰ **Definition:**  
Add noise from a **Gaussian (bell curve) distribution** with standard deviation:

\[
\sigma = \frac{\sqrt{2 \ln(1.25/\delta)} \cdot \text{sensitivity}}{\varepsilon}
\]

ğŸ‘‰ **Meaning in practice:**  
- Similar to Laplace, but with bell-curve noise instead of spiky noise.  
- Allows a tiny failure probability Î´.  
- Friendlier for many repeated queries (composition).  

---

## 4. Post-Processing Invariance

ğŸ‘‰ **Definition:**  
If the output of a mechanism is DP, then **any further computation** on it is still DP.

ğŸ‘‰ **Why it matters:**  
Once noise is added, you can:  
- Round it  
- Plot it  
- Feed it into another model  

â€¦and the DP guarantee remains intact.  
**You canâ€™t unblur it.**

---

## 5. Composition

ğŸ‘‰ **Definition:**  
Each DP query spends privacy budget. Running multiple queries **adds up** the cost.

- **Sequential composition (same users):**  
  If M1 is (Îµâ‚, Î´â‚)-DP and M2 is (Îµâ‚‚, Î´â‚‚)-DP, together they are  
  (Îµâ‚ + Îµâ‚‚, Î´â‚ + Î´â‚‚)-DP.

- **Parallel composition (disjoint users):**  
  If queries touch different people, the cost is just the **max**, not the sum.

ğŸ‘‰ **Why it matters:**  
- You have a **privacy wallet**.  
- Each query spends some Îµ (and Î´).  
- Too many queries â†’ budget runs out.  
- Forces you to plan carefully.

---

## 6. Quick Analogy

- **Sensitivity** = ruler: how much one person can change the answer.  
- **Laplace / Gaussian** = noise machines to blur the answer.  
- **Post-processing** = once blurred, it stays blurred.  
- **Composition** = wallet: every query spends a bit of your budget.

---

âœ… **Chapter 2 takeaway:**  
DP gives you tools to **measure impact (sensitivity)**, **add just enough noise (Laplace/Gaussian)**, and **track your budget (composition)**, while guaranteeing that once noise is added, privacy protection sticks (**post-processing**).
